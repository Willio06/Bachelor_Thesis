# README for Bachelor Thesis Repository
_generated by ChatGPT4_
I put this tag here, not an accident.
## Overview

This repository contains supplementary materials for the bachelor thesis titled **"Optimalisatietechnieken voor Neurale Netwerken"** by **Sam Beunckens** and **Tuur Willio**. The thesis explores theoretical and practical aspects of neural network optimization, including hyperparameter tuning, gradient methods, and activation functions.

---

## Contents

### 1. **Data**
- **Datasets** used for experiments

### 2. **Scripts**
- Python scripts used for:
  - Building and training feed-forward neural networks.
  - Hyperparameter optimization techniques (e.g., grid search, random search, Bayesian optimization).
  - Implementation of gradient methods such as stochastic gradient descent.

### 3. **Images**
- Visualizations and performance graphs included in the thesis.
- Supplementary diagrams of neural network architectures and optimization processes.

### 4. **GeoGebra Files**
- Interactive GeoGebra files illustrating mathematical concepts and proofs from the thesis, including:
  - Universal Approximation Theorem.
  - Gradient method visualizations.

---
## Introduction (Dutch)
In ons hedendaags leven valt het haast niet te ontwijken: Artifici¨ele Intelligentie. Dezer
dagen kent deze term vele variaties en toepassingen. Deze evolutie van artifici¨ele intelligentie
is sterk gefundeerd op feed-forward neurale netwerken. Terwijl deze neurale netwerken al
enkele decennia meegaan zijn deze nog altijd sterk van belang in nieuwe vormen van artifici¨ele
intelligentie. Ze vormen wel degelijk een basis. In deze paper nemen we deze onder de loep
en bekijken hoe goed deze juist zijn met hun benaderende eigenschappen. We bespreken hier
ten eerste hoe neurale netwerk eigenlijk in elkaar zitten en kunnen convergeren naar een punt
dat optimaal is. We zullen dan een neuraal netwerk zijn potentieel uitgebreid bespreken in
de vorm van de Universele Approximatie Stelling. Een stelling die in literatuur veel vormen
kent, met elks verschillende voorwaarden en gevolgen. Vele draaien rond abstracte idee¨en
in de analyse. We zullen zelf proberen een algemene, vatbare versie hier van te cre¨eren dat
niet te diep ingaat op abstracte theorie. Dit namelijk op basis van een visueel idee. Hierna
zullen we verder gaan naar hyperparameters voor neurale netwerken, een term die de lezer
nog zal tegenkomen. Een neuraal netwerk is namelijk een bundel van trainbare parameters
en parameters om deze vorige te trainen. Net zoals deze trainbare aangepast kunnen worden
naar keuze en dus naar een optimale oplossing, kunnen we ook deze hyperparameters, de
parameters van de parameters, instellen zodat alle vorige processen effici¨enter verlopen.
## Usage

### Running Scripts
1. Clone the repository:  
   ```bash
   git clone https://github.com//Willio06/Bachelor_Thesis.git
2. Install Python3 
3. Install Geogebra Classic

## Citation
If you use any materials from this repository, please cite the thesis as follows:
```
Beunckens, S., & Willio, T. (2024). Optimalisatietechnieken voor Neurale Netwerken. Universiteit Hasselt, Faculteit Wetenschappen.
```
LaTeX `.bib` file example
```
@article{BachThesisBeunckensWillio,
author = {Sam Beunckes, Tuur Willip},
year = {2024},
pages = {59},
title = {OPTIMALISATIETECHNIEKEN VOOR NEURALE NETWERKEN},
addres = {Universiteit Hasselt, Faculteit Wetenschappen}
}
```


# Code & extra Repo
https://github.com/SamBe-2158037/bachelorProef
